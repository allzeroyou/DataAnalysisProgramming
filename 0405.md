# Preview
참고: 할리스 홈페이지(https://www.hollys.co.kr/)
html 화면을 구성하는 요소를 명시한 문서

scrapy: 파이썬 프레임워크

과제면제권 사용할 때 과제게시판에 면제권 pdf 올리기

할리스 매장 검색

```
https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=1&sido=&gugun=&store=
https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=2&sido=&gugun=&store=
https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=3&sido=&gugun=&store=
https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=54&sido=&gugun=&store=
```
매장 검색 url 패턴화
웹 페이지에 html 정보는 정적임.
정적인 파일이 있고 정보를 보내주기 때문에, 인터랙티브 웹 페이지 (동적-자바스크립트)와 반대임.

tbody> tr > td 순으로 정보를 가져오면 됨!

비주얼스튜디오 코드 => 가상환경 불러오려면 터미널 환경 세팅이 필수라 
파이참으로 실습 진행.

```python
from bs4 import BeautifulSoup
import urllib.request
import pandas as pd
import requests
import datetime

#[CODE 1]
def hollys_store(result):
    result_test = []
    for page in range(1,55): # 54개의 매장 정보 페이지
        Hollys_url = f'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo={page}&sido=&gugun=&store='
        print(Hollys_url)
        #html = urllib.request.urlopen(Hollys_url) # url 라이브러리 사용
        #html = urllib.request.urlopen(Hollys_url).read()

        response =  requests.get(Hollys_url) # request 사용
        # urllib vs request
        # urlib을 사용 시 .read() 덧붙여 사용할 것

        if(response.status_code == 200):
            html = response.content
            soupHollys = BeautifulSoup(html, 'html.parser') # html 정보를 파싱. tag태그 정보 요청
            # select, find 골라서 사용 가능
            tag_tbody = soupHollys.find('tbody')
            # tbody는 하나의 태그라서 find 해서 걸러옴.
            # tbody 안에 10개의 tr 있음(리스트 형태로 넘어옴)
            for store in tag_tbody.find_all('tr'):
                store_td = store.find_all('td')
                store_name = store_td[1].string # 매장명
                store_sido = store_td[0].string # 지역
                store_address = store_td[3].string # 주소
                store_phone = store_td[5].string # 전화번호
                # 없는 값 가져오는 방식 ) string: none전달, get_text: 값이 없음
                # 교재) result.append([store_name] + [store_sido] + [store_address] + [store_phone])
                result_test.append([store_name, store_sido, store_address, store_phone])
                # 위처럼 배열로 만들면 된다.
        """    
        soupHollys = BeautifulSoup(html, 'html.parser')
        tag_tbody = soupHollys.find('tbody')
        for store in tag_tbody.find_all('tr'):
            store_td = store.find_all('td')
            store_name = store_td[1].string
            store_sido = store_td[0].string
            store_address = store_td[3].string
            store_phone = store_td[5].string
            result.append([store_name]+[store_sido]+[store_address]+[store_phone])
            result_test.append([store_name, store_sido, store_address, store_phone])
        """

    return

#[CODE 0]
def main():
    result = []
    # 전역변수는 가급적 안 잡는 것이 좋음.
    # 위처럼 지역변수로 하자.
    print('Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')
    hollys_store(result)   #[CODE 1] 호출 
    hollys_tbl = pd.DataFrame(result, columns=('store', 'sido-gu', 'address','phone'))
    hollys_tbl.to_csv('hollys.csv', encoding='cp949', mode='w', index=True)
    # ms에서의 한글 인코딩 방식: cp949
    # utf-8은 ms엑셀에서 깨질 것
       
if __name__ == '__main__':
     main()
```