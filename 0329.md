# preview
네이버 오픈 api를 활용한 크롤링

주피터 노트북이 아닌 파이참을 쓰는 이유: 객체의 속성값, 하위 속성 ... => 계속 확인 가능

단편적으로 한 줄 한 줄 씩 실행하려면 주피터 노트북이 적절하나, 디버깅을 하기위해 파이참을 사용.

https://developers.naver.com/docs/serviceapi/search/news/news.md#%EB%89%B4%EC%8A%A4


```python
def getRequestUrl(url):    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", client_id)
    req.add_header("X-Naver-Client-Secret", client_secret)
    
    try: 
        response = urllib.request.urlopen(req)
        if response.getcode() == 200:
            print ("[%s] Url Request Success" % datetime.datetime.now())
            return response.read().decode('utf-8')
    except Exception as e:
        print(e)
        print("[%s] Error for URL : %s" % (datetime.datetime.now(), url))
        return None
```

url말고, header 정보에 정보를 붙여 전송 가능.
req라는 url을 호출하는 객체를 만들어 호출
response는 file과 유사한 객체.
원하는 데이터를 뽑으려면 read를 해주고 unicode(utf-8)로 변환한다.

```python
def main():
    node = 'news'   # 크롤링 할 대상
    srcText = input('검색어를 입력하세요: ')
    cnt = 0
    jsonResult = []

    jsonResponse = getNaverSearch(node, srcText, 1, 100)  #[CODE 2]
                              #디폴트는 10, 최대 100개까지 가능
    total = jsonResponse['total']
```

딕셔너리 형태의 jsonResponse.
딕셔너리 => json 파일로 변환 후 저장

```python
    while ((jsonResponse != None) and (jsonResponse['display'] != 0)):         
        for post in jsonResponse['items']: #item = 뉴스 url
            cnt += 1
            getPostData(post, jsonResult, cnt)  #[CODE 3]       
        
        start = jsonResponse['start'] + jsonResponse['display']
        jsonResponse = getNaverSearch(node, srcText, start, 100)  #[CODE 2]
```
item을 하나씩 읽어온다.
getPostData를 통해 json 형태로 저장(?)

jsonResponse의 start = 1
1000개까지만 네이버 openAPI가 지원하니, 1001가 되는 순간 반복문을 빠져나온다.

키워드를 가지고 뉴스 검색 해 임의의 JSON 형태로 파일이 저장됨.


크롤링을 했던 사례 어떤 사이트를 어떤 방법으로 크롤링해서 어떤 데이터를 가져왔다~ 지원은 선착순 메일로-3명 (혜택은 2회 과제면제권 )


서버 같은 걸 만든 후 서비스를 하고 싶음 => 사용자에게 어떻게 요청, 응답을 할지 (모든 시스템에 있으나 공개가 되지 않음. 공개가 된 것은 오픈 API임)

![](https://images.velog.io/images/allzeroyou/post/07429185-451c-407d-99ab-0f5939e75d9c/image.png)
미리보기 클릭 시 아직 API가 승인되지 않았다는 오류 발생함

API 환경 또는 API 호출 조건에 따라 인증키가 적용되는 방식이 다를 수 있습니다.
포털에서 제공되는 Encoding/Decoding 된 인증키를 적용하면서 구동되는 키를 사용하시기 바랍니다.
* 향후 포털에서 더 명확한 정보를 제공하기 위해 노력하겠습니다.
=> html에서 직접 호출하거나
url을 만들어 직접 호출: 일반 인증키(encoding)
request 모듈=> 함수의 힘을 빌려 사용: 일반 인증키(decoding)


> Ch5_openapi_tour 파일 클릭


![](https://images.velog.io/images/allzeroyou/post/d9b789df-b9c8-4588-a1c2-b9e67b1deb20/image.png)
해당하는 가상환경에서 가져와서 열린 가상환경 터미널


![](https://images.velog.io/images/allzeroyou/post/f3a0e7c8-e4ae-424b-931f-7a8135129f53/image.png)
getTourismStatsService에는 입,출국/ start와 종료시점을 넣으면 jsonResult등이 나온다.

F7을 눌러서 한 줄씩 실행시킨다.

![](https://images.velog.io/images/allzeroyou/post/cfd4a327-e7c3-44f2-9267-7db98260e24c/image.png)
f9을 눌러 jump 가능
![](https://images.velog.io/images/allzeroyou/post/26f093cb-f4ef-4279-9a60-03f7412e94b9/image.png)
네이버 api와 달리 헤더정보가 없다.

내용을 읽어와야 해독이 가능하다.

```
return json.loads(retData)
```

json 데이터를 불러옴

item에 검색된 데이터(딕셔너리)를 잘 확인해보자.(결괏값)

response header의 메시지가 ok인지 확인.

natName = natName.replace(' ', '')
을 통해 국가명에 띄어쓰기된 것을 없애준다.
```python
jsonResult.append({'nat_name': natName, 'nat_cd': nat_cd, 'yyyymm': yyyymm, 'visit_cnt': num})
result.append([natName, nat_cd, yyyymm, num])
```               
              
append를 통해 딕셔너리에 값을 계속 추가해준다.

![](https://images.velog.io/images/allzeroyou/post/65ac95ea-0c36-45ca-98d0-9d3a3ed3dd9c/image.png)
만약 데이터가 더 이상 없다면, if문의 걸려 break 된다.

![](https://images.velog.io/images/allzeroyou/post/0b7b4f3d-6d61-4c57-84c1-9b429d25deb2/image.png)


>openapi.pharmact.py 파일 열기

requests.get 메소드사용
파라미터는 완전한 텍스트가 아닌 딕셔너리 형태임 => url을 직접 지정하지 않으니 decoding을 사용하면 된다!

![](https://images.velog.io/images/allzeroyou/post/6d62cfac-10ea-4d78-a51d-ca7885d2f553/image.png)
text => xml => 딕셔너리

그러면 openapi_tour.py 파일을 요 형식으로 바꿔보자.
![](https://images.velog.io/images/allzeroyou/post/d5367e12-ddde-4860-9783-ec4201eef59a/image.png)
key를 decoding key로 변환한 후

scalar values=> 1차원 데이터라 pandas에서 dataframe이 아닌 **series**로 바꾸면 오류가 해결될 것.

이렇게 하지 않고 스칼라든 시리즈든 바꾸려면?
`
from pandas.io.json import json_normalize
`
json_normalize을 이용한다.
1차원, 2차원 상관없이 데이터프레임으로 바꿔준다!

데이터프레임을 만들때 컬럼을 직접 주거나 json_normailize을 이용하면 이 아이가 컬럼을 직접 만들어준다.

> 과제
네이버 크롤링 => requests 모듈을 써서 해보기
네이버, tour => url 직접
pharmacy => requests 모듈 사용했음
데이터 뽑아와서 csv로 저장하기

네이버는 html header에 정보를 담아 보냈음.
url+파라미터 정보를 텍스트로 만든 정보를 보냄

> 
헤더로 넘기기~
결과를 해당되는 (주석 ######### or 하이라이트) 코드를 올리기(직접 실행해볼 예정)