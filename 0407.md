중간고사
크롤링 > pandas > 데이터 정보를 이용(min/max, 특정 속성에 대해 count 등)

# 정적 웹페이지 크롤링(hollys)
![](https://velog.velcdn.com/cloudflare/allzeroyou/03465b24-6cfa-41b2-afa0-08f4b3e85627/image.png)

```
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=1&sido=&gugun=&store='

response = requests.get(url)

if response.status_code == 200:
    html = response.content # html 정보를 가져옴
    soup = BeautifulSoup(html, 'html.parser')

    tbody = soup.find_all('tbody') # tbody가 몇갠지 확인을 먼저하기
    print(len(tbody))

else :
    print(response.status_code)
```

결과는 1이다.
따라서 find_all을 하기보다는(1개 요소이기에, 리스트로 출력하는것이 불편) find을 이용하자.

```
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo=1&sido=&gugun=&store='

response = requests.get(url)

if response.status_code == 200:
    html = response.content # html 정보를 가져옴
    soup = BeautifulSoup(html, 'html.parser')

    tbody = soup.find('tbody') # tbody가 몇갠지 확인을 먼저하기
    tr_list = tbody.find_all('tr')
    print(len(tr_list))
else :
    print(response.status_code)


```

참고
https://wikidocs.net/85739

전체 실습코드
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

results = []

for pageNum in range(1, 55):
    # 문자열 fotmating: f-string 이용
    url = f'https://www.hollys.co.kr/store/korea/korStore2.do?pageNo={pageNum}&sido=&gugun=&store='
    response = requests.get(url)
    #print('-'*20, pageNum)

    if response.status_code == 200:
        html = response.content
        soup = BeautifulSoup(html, 'html.parser')
        tbody = soup.find('tbody')
        tr_list = tbody.find_all('tr')

        for td in tr_list:
            td_list = td.find_all('td')
            loc = td_list[0].text # 지역
            name = td_list[1].text # 이름
            addr = td_list[3].string # 주소
            tel = td_list[5].string # 전화번호
            results.append([loc, name, addr, tel])
            #print(loc, name, addr, tel)

    else:
        print(response.status_code)

pdResult=pd.DataFrame(results, columns=['지역','매장명','주소','전화번호'])
pdResult.to_csv('hollys_stores.csv', encoding='cp949')
print(pdResult)
```
# 동적 웹사이트 크롤링(coffeebean)

selenium을 이용하겠음.
1. pip install selenium
![](https://velog.velcdn.com/images/allzeroyou/post/c234f832-a487-4761-b4a6-360af9f9c2c5/image.png)

```python
pip install selenium==3.141.0
```
selenium은 3, 4버전이 있는데
일반적으로 많이 쓰이는 3.141.0 버전을 사용하겠음.


![](https://velog.velcdn.com/cloudflare/allzeroyou/263bc110-c5da-45b4-8382-ef07e28eb9f3/image.png)
select는 결과값이 하나이든 두개이든 리스트로 반환함.
select는 경로를 지정할 수 있음('div > tbody ...')

2. chromedriver 다운로드
https://chromedriver.chromium.org/downloads

현재 실행중인 프로젝트 폴더에 chromedriver을 옮겨논다.

![](https://velog.velcdn.com/images/allzeroyou/post/0cdd291f-de87-4cb7-bd0a-9e6b59ed6227/image.png)
![](https://velog.velcdn.com/images/allzeroyou/post/f377b485-2f4c-49c9-beb0-d4204c669db6/image.png)

div.store_txt 안에 table 정보가 있고, table안에 td, tr 정보가 있음.